{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "u-Y6Po8Ijvlb",
    "outputId": "23e7a395-e9c4-49ba-e230-25ae6d23b726"
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "cell": {
        "!": "OSMagics",
        "HTML": "Other",
        "SVG": "Other",
        "bash": "Other",
        "bigquery": "Other",
        "capture": "ExecutionMagics",
        "debug": "ExecutionMagics",
        "file": "Other",
        "html": "DisplayMagics",
        "javascript": "DisplayMagics",
        "js": "DisplayMagics",
        "latex": "DisplayMagics",
        "perl": "Other",
        "prun": "ExecutionMagics",
        "pypy": "Other",
        "python": "Other",
        "python2": "Other",
        "python3": "Other",
        "ruby": "Other",
        "script": "ScriptMagics",
        "sh": "Other",
        "shell": "Other",
        "svg": "DisplayMagics",
        "sx": "OSMagics",
        "system": "OSMagics",
        "time": "ExecutionMagics",
        "timeit": "ExecutionMagics",
        "writefile": "OSMagics"
       },
       "line": {
        "alias": "OSMagics",
        "alias_magic": "BasicMagics",
        "autocall": "AutoMagics",
        "automagic": "AutoMagics",
        "autosave": "KernelMagics",
        "bookmark": "OSMagics",
        "cat": "Other",
        "cd": "OSMagics",
        "clear": "KernelMagics",
        "colors": "BasicMagics",
        "config": "ConfigMagics",
        "connect_info": "KernelMagics",
        "cp": "Other",
        "debug": "ExecutionMagics",
        "dhist": "OSMagics",
        "dirs": "OSMagics",
        "doctest_mode": "BasicMagics",
        "ed": "Other",
        "edit": "KernelMagics",
        "env": "OSMagics",
        "gui": "BasicMagics",
        "hist": "Other",
        "history": "HistoryMagics",
        "killbgscripts": "ScriptMagics",
        "ldir": "Other",
        "less": "KernelMagics",
        "lf": "Other",
        "lk": "Other",
        "ll": "Other",
        "load": "CodeMagics",
        "load_ext": "ExtensionMagics",
        "loadpy": "CodeMagics",
        "logoff": "LoggingMagics",
        "logon": "LoggingMagics",
        "logstart": "LoggingMagics",
        "logstate": "LoggingMagics",
        "logstop": "LoggingMagics",
        "ls": "Other",
        "lsmagic": "BasicMagics",
        "lx": "Other",
        "macro": "ExecutionMagics",
        "magic": "BasicMagics",
        "man": "KernelMagics",
        "matplotlib": "PylabMagics",
        "mkdir": "Other",
        "more": "KernelMagics",
        "mv": "Other",
        "notebook": "BasicMagics",
        "page": "BasicMagics",
        "pastebin": "CodeMagics",
        "pdb": "ExecutionMagics",
        "pdef": "NamespaceMagics",
        "pdoc": "NamespaceMagics",
        "pfile": "NamespaceMagics",
        "pinfo": "NamespaceMagics",
        "pinfo2": "NamespaceMagics",
        "pip": "Other",
        "popd": "OSMagics",
        "pprint": "BasicMagics",
        "precision": "BasicMagics",
        "profile": "BasicMagics",
        "prun": "ExecutionMagics",
        "psearch": "NamespaceMagics",
        "psource": "NamespaceMagics",
        "pushd": "OSMagics",
        "pwd": "OSMagics",
        "pycat": "OSMagics",
        "pylab": "PylabMagics",
        "qtconsole": "KernelMagics",
        "quickref": "BasicMagics",
        "recall": "HistoryMagics",
        "rehashx": "OSMagics",
        "reload_ext": "ExtensionMagics",
        "rep": "Other",
        "rerun": "HistoryMagics",
        "reset": "NamespaceMagics",
        "reset_selective": "NamespaceMagics",
        "rm": "Other",
        "rmdir": "Other",
        "run": "ExecutionMagics",
        "save": "CodeMagics",
        "sc": "OSMagics",
        "set_env": "OSMagics",
        "shell": "Other",
        "store": "StoreMagics",
        "sx": "OSMagics",
        "system": "OSMagics",
        "tb": "ExecutionMagics",
        "tensorflow_version": "Other",
        "time": "ExecutionMagics",
        "timeit": "ExecutionMagics",
        "unalias": "OSMagics",
        "unload_ext": "ExtensionMagics",
        "who": "NamespaceMagics",
        "who_ls": "NamespaceMagics",
        "whos": "NamespaceMagics",
        "xdel": "NamespaceMagics",
        "xmode": "BasicMagics"
       }
      },
      "text/plain": [
       "Available line magics:\n",
       "%alias  %alias_magic  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %colors  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %man  %matplotlib  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %pip  %popd  %pprint  %precision  %profile  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %shell  %store  %sx  %system  %tb  %tensorflow_version  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n",
       "\n",
       "Available cell magics:\n",
       "%%!  %%HTML  %%SVG  %%bash  %%bigquery  %%capture  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%shell  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n",
       "\n",
       "Automagic is ON, % prefix IS NOT needed for line magics."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HHQIK_L_ieh5"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "def read_devnagri_text(text):\n",
    "    text = text.replace('\\u200c', '')\n",
    "    text = text.replace('\\u200d', '')\n",
    "    text = text.replace('\\ufeff', '')\n",
    "    text = text.replace('\\xad', '')\n",
    "    text = text.replace(u'-','')\n",
    "    text = text.replace('-','')\n",
    "    text = text.replace('–','')\n",
    "    text = text.replace('—','')\n",
    "    text = text.replace(' : ','')\n",
    "    text = text.replace('’','')\n",
    "    text = text.replace('“','')\n",
    "    text = text.replace('”','')\n",
    "    text = text.replace('!','')\n",
    "    text = text.replace('%','')\n",
    "    text = text.replace('&','')\n",
    "    text = text.replace('*','')\n",
    "    text = text.replace('+','')\n",
    "    text = text.replace('?','')\n",
    "    text = text.replace('/','')\n",
    "    text = text.replace(';','')\n",
    "    text = text.replace('{','')\n",
    "    text = text.replace('}','')\n",
    "    text = text.replace('=','')\n",
    "    text = text.replace(',','')\n",
    "    text = text.replace('\\'','')\n",
    "    text = text.replace('\"','')\n",
    "    text = text.replace(u'.','')\n",
    "    text = text.replace('॥','')\n",
    "    text = text.replace(')','')\n",
    "    text = text.replace('(','')\n",
    "    text = text.replace('[','')\n",
    "    text = text.replace(']','')\n",
    "    text = text.replace('<','')\n",
    "    text = text.replace('>','')\n",
    "    text = text.replace('`','')\n",
    "    text = text.replace('‘','')\n",
    "    text = text.replace('#','')\n",
    "    text = text.replace('।', '')\n",
    "    text = text.replace('|', '')\n",
    "    text = re.sub('[0-9a-zA-Z]', '', text)\n",
    "    text = re.sub('[०-९]', '', text)\n",
    "    text = text.strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzCweqR4j-pM"
   },
   "outputs": [],
   "source": [
    "!pip install indic_transliteration\n",
    "!pip install devnagri_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Mfxt-Fpzj4Kw"
   },
   "outputs": [],
   "source": [
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import SchemeMap, SCHEMES, transliterate\n",
    "import numpy as np\n",
    "\n",
    "swaras = ['a', 'A', 'i', 'I', 'u', 'U', 'e', 'E', 'o', 'O', 'f', 'F', 'x', 'X']\n",
    "vyanjanas = ['k', 'K', 'g', 'G', 'N', \n",
    "             'c', 'C', 'j', 'J', 'Y',\n",
    "             'w', 'W', 'q', 'Q', 'R',\n",
    "             't', 'T', 'd', 'D', 'n',\n",
    "             'p', 'P', 'b', 'B', 'm',\n",
    "             'y', 'r', 'l', 'v','S', 'z', 's', 'h', 'L', '|']\n",
    "others = ['H', 'Z', 'V', 'M', '~', '/', '\\\\', '^', '\\'']\n",
    "\n",
    "slp1charlist = swaras + vyanjanas + others\n",
    "\n",
    "def remove_nonslp1_chars(word):\n",
    "    newword = ''\n",
    "    for char in word:\n",
    "        if char in slp1charlist:\n",
    "            newword = newword + char\n",
    "    return newword\n",
    "\n",
    "def get_sandhi_dataset(datafile):\n",
    "    word1list = []\n",
    "    word2list = []\n",
    "    outputlist = []\n",
    "\n",
    "    with open(datafile) as fp:\n",
    "        tests = fp.read().splitlines()\n",
    "    \n",
    "    total = 0\n",
    "    maxlen = 0\n",
    "\n",
    "    for test in tests:\n",
    "        #print(test)\n",
    "    \n",
    "        if(test.find('=>') == -1):\n",
    "            continue;\n",
    "        inout = test.split('=>')\n",
    "        words = inout[1].split('+')\n",
    "    \n",
    "        if(len(words) != 2):\n",
    "            continue\n",
    "    \n",
    "        word1 = words[0].strip()\n",
    "        word1 = read_devnagri_text(word1)\n",
    "        slp1word1 = transliterate(word1, sanscript.DEVANAGARI, sanscript.SLP1)\n",
    "        slp1word1 = remove_nonslp1_chars(slp1word1)\n",
    "    \n",
    "        word2 = words[1].strip()\n",
    "        word2 = read_devnagri_text(word2)\n",
    "        slp1word2 = transliterate(word2, sanscript.DEVANAGARI, sanscript.SLP1)\n",
    "        slp1word2 = remove_nonslp1_chars(slp1word2)\n",
    "    \n",
    "        expected = inout[0].strip()\n",
    "        expected = read_devnagri_text(expected)\n",
    "        slp1expected = transliterate(expected, sanscript.DEVANAGARI, sanscript.SLP1)\n",
    "        slp1expected = remove_nonslp1_chars(slp1expected)\n",
    "\n",
    "        if slp1word1 and slp1word2 and slp1expected:\n",
    "            total = total + 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        fwl = 4\n",
    "        swl = 2\n",
    "\n",
    "        if len(slp1word1) > fwl:\n",
    "            excess = len(slp1word1) - fwl\n",
    "            slp1word1 = slp1word1[-fwl:]\n",
    "            slp1expected = slp1expected[excess:]\n",
    "        if len(slp1word2) > swl:\n",
    "            excess = len(slp1word2) - swl\n",
    "            slp1word2 = slp1word2[0:swl]\n",
    "            slp1expected = slp1expected[:-excess]\n",
    "\n",
    "        if(maxlen < len(slp1expected)):\n",
    "            maxlen = len(slp1expected)\n",
    "\n",
    "        difflen = len(slp1expected) - (len(slp1word1) + len(slp1word2))\n",
    "\n",
    "        if difflen < 2 and difflen > -3:\n",
    "            word1list.append(slp1word1)\n",
    "            word2list.append(slp1word2)\n",
    "            outputlist.append(slp1expected)\n",
    "\n",
    "        #print(slp1expected + ' => ' + slp1word1 + ' + ' + slp1word2)\n",
    "        #print(expected + ' => ' + word1 + ' + ' + word2)\n",
    "\n",
    "    #print(total)\n",
    "    #print(maxlen)\n",
    "    return word1list, word2list, outputlist\n",
    "\n",
    "def get_xy_data(datafile):\n",
    "    w1l, w2l, ol = get_sandhi_dataset(datafile)\n",
    "    return w1l, w2l, ol\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rloqFIQ-krCk",
    "outputId": "b582bde5-6149-42d9-a34c-e16acb64bf0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sandhi dataset created\n",
      "Number of samples: 64822\n",
      "Number of unique tokens: 53\n",
      "Max sequence length for inputs: 7\n",
      "Max sequence length for outputs: 9\n",
      "Epoch 1/5\n",
      "811/811 [==============================] - 18s 16ms/step - loss: 1.8890 - accuracy: 0.4972 - val_loss: 1.3056 - val_accuracy: 0.6429\n",
      "Epoch 2/5\n",
      "811/811 [==============================] - 11s 14ms/step - loss: 0.9447 - accuracy: 0.7451 - val_loss: 0.6767 - val_accuracy: 0.8243\n",
      "Epoch 3/5\n",
      "811/811 [==============================] - 11s 14ms/step - loss: 0.5208 - accuracy: 0.8635 - val_loss: 0.4181 - val_accuracy: 0.8906\n",
      "Epoch 4/5\n",
      "811/811 [==============================] - 12s 14ms/step - loss: 0.3430 - accuracy: 0.9144 - val_loss: 0.2932 - val_accuracy: 0.9271\n",
      "Epoch 5/5\n",
      "811/811 [==============================] - 11s 14ms/step - loss: 0.2544 - accuracy: 0.9378 - val_loss: 0.2618 - val_accuracy: 0.9368\n",
      "\n",
      "/content/Astaadhyaayii.txt\n",
      "Passed: 580/1574, 36.84879288437103\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "#cimport sandhi_data_prepare as sdp\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 5  # Number of epochs to train for.\n",
    "latent_dim = 16  # Latent dimensionality of the encoding space.\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "characters = set()\n",
    "\n",
    "w1l, w2l, ol = get_xy_data(\"/content/sandhiset.txt\")\n",
    "\n",
    "print(\"Sandhi dataset created\")\n",
    "\n",
    "for i in range(len(w1l)):\n",
    "    input_text = w1l[i] + '+' + w2l[i]\n",
    "    target_text = ol[i]\n",
    "    # We use \"&\" as the \"start sequence\" character for the targets, and \"$\" as \"end sequence\" character.\n",
    "    target_text = '&' + target_text + '$'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in characters:\n",
    "            characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in characters:\n",
    "            characters.add(char)\n",
    "\n",
    "# Split the training and testing data\n",
    "input_texts, X_tests, target_texts, Y_tests = train_test_split(input_texts, target_texts, test_size=0.2, random_state=1)\n",
    "\n",
    "# Using '*' for padding \n",
    "characters.add('*')\n",
    "\n",
    "characters = sorted(list(characters))\n",
    "num_tokens = len(characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique tokens:', num_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "token_index = dict([(char, i) for i, char in enumerate(characters)])\n",
    "\n",
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_tokens), dtype='float32')\n",
    "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_tokens), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_tokens), dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, token_index['*']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, token_index['*']] = 1.\n",
    "    decoder_target_data[i, t:, token_index['*']] = 1.\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_tokens))\n",
    "encoder = Bidirectional(LSTM(latent_dim, return_state=True))\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_tokens))\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "\n",
    "# Save model\n",
    "model.save('bis2s.h5')\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, token_index['&']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '$' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "test_file_list = [\"/content/Astaadhyaayii.txt\"]\n",
    "\n",
    "\n",
    "for test_file in test_file_list:\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    \n",
    "    if test_file == \"testset\":\n",
    "        input_texts = X_tests\n",
    "        target_texts = Y_tests\n",
    "        for i in range(len(target_texts)):\n",
    "            target_texts[i] = target_texts[i][1:-1]\n",
    "    else:\n",
    "        w1l, w2l, ol = get_xy_data(test_file)\n",
    "    \n",
    "        for i in range(len(w1l)):\n",
    "            input_text = w1l[i] + '+' + w2l[i]\n",
    "            target_text = ol[i]\n",
    "            input_texts.append(input_text)\n",
    "            target_texts.append(target_text)\n",
    "    \n",
    "    encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_tokens), dtype='float32')\n",
    "    \n",
    "    for i, input_text in enumerate(input_texts):\n",
    "        for t, char in enumerate(input_text):\n",
    "            if char not in token_index:\n",
    "                continue\n",
    "            encoder_input_data[i, t, token_index[char]] = 1.\n",
    "        encoder_input_data[i, t + 1:, token_index['*']] = 1.\n",
    "    \n",
    "    # Next: inference mode (sampling).\n",
    "    # Here's the drill:\n",
    "    # 1) encode input and retrieve initial decoder state\n",
    "    # 2) run one step of decoder with this initial state\n",
    "    # and a \"start of sequence\" token as target.\n",
    "    # Output will be the next target token\n",
    "    # 3) Repeat with the current target token and current states\n",
    "    \n",
    "    # Define sampling models\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = Input(shape=(latent_dim*2,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim*2,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    # Reverse-lookup token index to decode sequences back to something readable.\n",
    "    reverse_input_char_index = dict((i, char) for char, i in token_index.items())\n",
    "    reverse_target_char_index = dict((i, char) for char, i in token_index.items())\n",
    "    \n",
    "    total = len(encoder_input_data)\n",
    "    passed = 0\n",
    "    for seq_index in range(len(encoder_input_data)):\n",
    "        # Take one sequence (part of the training set)\n",
    "        # for trying out decoding.\n",
    "        input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "        decoded_sentence = decode_sequence(input_seq)\n",
    "        decoded_sentence = decoded_sentence.strip()\n",
    "        decoded_sentence = decoded_sentence.strip('$')\n",
    "        if decoded_sentence == target_texts[seq_index]:\n",
    "            passed = passed + 1\n",
    "        \"\"\"\n",
    "        else:\n",
    "            print(str(seq_index)+'/'+str(total))\n",
    "            print('-')\n",
    "            print('Input sentence:   ', input_texts[seq_index])\n",
    "            print('Decoded sentence: ', decoded_sentence)\n",
    "            print('Expected sentence:', target_texts[seq_index])\n",
    "        \"\"\"\n",
    "    print(\"\\n\"+test_file)\n",
    "    print(\"Passed: \"+str(passed)+'/'+str(total)+', '+str(passed*100/total))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
